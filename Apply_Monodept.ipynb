{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apply Monodept.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2t9O5OsJ0UMq",
        "Knb75W9ImFmE"
      ],
      "authorship_tag": "ABX9TyNGicbxfkjDNwaUzmxBeGR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trantoai1/GraduatedProject/blob/main/Apply_Monodept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui6kQPHkPsJH",
        "outputId": "4ca04cc3-9873-45b0-da00-bdbe89b52418"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1A3_1MtSCY"
      },
      "source": [
        "# Prepare\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t9O5OsJ0UMq"
      },
      "source": [
        "## install requirements for prunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmhKfq4rk0K1"
      },
      "source": [
        "for z in *.zip; do unzip $z && rm $z; done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5PsX9qU0eQc"
      },
      "source": [
        "%cd /content/drive/MyDrive/SSL/monodepth2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVH4O0kyO-PH"
      },
      "source": [
        "!pip install tensorboardX==1.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDGFgXqWl4xF"
      },
      "source": [
        "# Test with monodepth\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knb75W9ImFmE"
      },
      "source": [
        "##Options\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU-O3lAk6jYK",
        "outputId": "93eaf918-f476-459e-f8d7-b99f9530ac52"
      },
      "source": [
        "%cd /content/drive/MyDrive/SSL/monodepth2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SSL/monodepth2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zJX-qzNl-UI"
      },
      "source": [
        "# Copyright Niantic 2019. Patent Pending. All rights reserved.\n",
        "#\n",
        "# This software is licensed under the terms of the Monodepth2 licence\n",
        "# which allows for non-commercial use only, the full terms of which are made\n",
        "# available in the LICENSE file.\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import json\n",
        "from utils import AverageMeter\n",
        "from utils import *\n",
        "from kitti_utils import *\n",
        "from layers import *\n",
        "import datasets\n",
        "import networks\n",
        "from IPython import embed\n",
        "\n",
        "file_dir = '/content/drive/MyDrive/SSL/monodepth2/'  # the directory that options.py resides in\n",
        "\n",
        "\n",
        "class MonodepthOptions:\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser(description=\"Monodepthv2 options\")\n",
        "\n",
        "        # PATHS\n",
        "        self.parser.add_argument(\"--data_path\",\n",
        "                                 type=str,\n",
        "                                 help=\"path to the training data\",\n",
        "                                 default=os.path.join(file_dir, \"kitti_data\"))\n",
        "        self.parser.add_argument(\"--log_dir\",\n",
        "                                 type=str,\n",
        "                                 help=\"log directory\",\n",
        "                                 default='newmodel')\n",
        "\n",
        "        # TRAINING options\n",
        "        self.parser.add_argument(\"--model_name\",\n",
        "                                 type=str,\n",
        "                                 help=\"the name of the folder to save the model in\",\n",
        "                                 default=\"newmodel/mono_model/models/weights_3\")\n",
        "        self.parser.add_argument(\"--split\",\n",
        "                                 type=str,\n",
        "                                 help=\"which training split to use\",\n",
        "                                 choices=[\"eigen_zhou\", \"eigen_full\", \"odom\", \"benchmark\"],\n",
        "                                 default=\"eigen_zhou\")\n",
        "        self.parser.add_argument(\"--num_layers\",\n",
        "                                 type=int,\n",
        "                                 help=\"number of resnet layers\",\n",
        "                                 default=18,\n",
        "                                 choices=[18, 34, 50, 101, 152])\n",
        "        self.parser.add_argument(\"--dataset\",\n",
        "                                 type=str,\n",
        "                                 help=\"dataset to train on\",\n",
        "                                 default=\"kitti\",\n",
        "                                 choices=[\"kitti\", \"kitti_odom\", \"kitti_depth\", \"kitti_test\"])\n",
        "        self.parser.add_argument(\"--png\",\n",
        "                                 help=\"if set, trains from raw KITTI png files (instead of jpgs)\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--height\",\n",
        "                                 type=int,\n",
        "                                 help=\"input image height\",\n",
        "                                 default=192)\n",
        "        self.parser.add_argument(\"--width\",\n",
        "                                 type=int,\n",
        "                                 help=\"input image width\",\n",
        "                                 default=640)\n",
        "        self.parser.add_argument(\"--disparity_smoothness\",\n",
        "                                 type=float,\n",
        "                                 help=\"disparity smoothness weight\",\n",
        "                                 default=1e-3)\n",
        "        self.parser.add_argument(\"--scales\",\n",
        "                                 nargs=\"+\",\n",
        "                                 type=int,\n",
        "                                 help=\"scales used in the loss\",\n",
        "                                 default=[0, 1, 2, 3])\n",
        "        self.parser.add_argument(\"--min_depth\",\n",
        "                                 type=float,\n",
        "                                 help=\"minimum depth\",\n",
        "                                 default=0.1)\n",
        "        self.parser.add_argument(\"--max_depth\",\n",
        "                                 type=float,\n",
        "                                 help=\"maximum depth\",\n",
        "                                 default=100.0)\n",
        "        self.parser.add_argument(\"--use_stereo\",\n",
        "                                 help=\"if set, uses stereo pair for training\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--frame_ids\",\n",
        "                                 nargs=\"+\",\n",
        "                                 type=int,\n",
        "                                 help=\"frames to load\",\n",
        "                                 default=[0, -1, 1])\n",
        "\n",
        "        # OPTIMIZATION options\n",
        "        self.parser.add_argument(\"--batch_size\",\n",
        "                                 type=int,\n",
        "                                 help=\"batch size\",\n",
        "                                 default=12)\n",
        "        self.parser.add_argument(\"--learning_rate\",\n",
        "                                 type=float,\n",
        "                                 help=\"learning rate\",\n",
        "                                 default=1e-4)\n",
        "        self.parser.add_argument(\"--num_epochs\",\n",
        "                                 type=int,\n",
        "                                 help=\"number of epochs\",\n",
        "                                 default=20)\n",
        "        self.parser.add_argument(\"--scheduler_step_size\",\n",
        "                                 type=int,\n",
        "                                 help=\"step size of the scheduler\",\n",
        "                                 default=15)\n",
        "\n",
        "        # ABLATION options\n",
        "        self.parser.add_argument(\"--v1_multiscale\",\n",
        "                                 help=\"if set, uses monodepth v1 multiscale\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--avg_reprojection\",\n",
        "                                 help=\"if set, uses average reprojection loss\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--disable_automasking\",\n",
        "                                 help=\"if set, doesn't do auto-masking\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--predictive_mask\",\n",
        "                                 help=\"if set, uses a predictive masking scheme as in Zhou et al\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--no_ssim\",\n",
        "                                 help=\"if set, disables ssim in the loss\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--weights_init\",\n",
        "                                 type=str,\n",
        "                                 help=\"pretrained or scratch\",\n",
        "                                 default=\"pretrained\",\n",
        "                                 choices=[\"pretrained\", \"scratch\"])\n",
        "        self.parser.add_argument(\"--pose_model_input\",\n",
        "                                 type=str,\n",
        "                                 help=\"how many images the pose network gets\",\n",
        "                                 default=\"pairs\",\n",
        "                                 choices=[\"pairs\", \"all\"])\n",
        "        self.parser.add_argument(\"--pose_model_type\",\n",
        "                                 type=str,\n",
        "                                 help=\"normal or shared\",\n",
        "                                 default=\"separate_resnet\",\n",
        "                                 choices=[\"posecnn\", \"separate_resnet\", \"shared\"])\n",
        "\n",
        "        # SYSTEM options\n",
        "        self.parser.add_argument(\"--no_cuda\",\n",
        "                                 help=\"if set disables CUDA\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--num_workers\",\n",
        "                                 type=int,\n",
        "                                 help=\"number of dataloader workers\",\n",
        "                                 default=1)\n",
        "\n",
        "        # LOADING options\n",
        "        self.parser.add_argument(\"--load_weights_folder\",\n",
        "                                 type=str,\n",
        "                                 help=\"name of model to load\",\n",
        "                                 default=\"newmodel/mono_model/models/weights_3\")\n",
        "        self.parser.add_argument(\"--models_to_load\",\n",
        "                                 nargs=\"+\",\n",
        "                                 type=str,\n",
        "                                 help=\"models to load\",\n",
        "                                 default=[\"encoder\", \"depth\"])\n",
        "\n",
        "        # LOGGING options\n",
        "        self.parser.add_argument(\"--log_frequency\",\n",
        "                                 type=int,\n",
        "                                 help=\"number of batches between each tensorboard log\",\n",
        "                                 default=250)\n",
        "        self.parser.add_argument(\"--save_frequency\",\n",
        "                                 type=int,\n",
        "                                 help=\"number of epochs between each save\",\n",
        "                                 default=1)\n",
        "\n",
        "        # EVALUATION options\n",
        "        self.parser.add_argument(\"--eval_stereo\",\n",
        "                                 help=\"if set evaluates in stereo mode\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--eval_mono\",\n",
        "                                 help=\"if set evaluates in mono mode\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--disable_median_scaling\",\n",
        "                                 help=\"if set disables median scaling in evaluation\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--pred_depth_scale_factor\",\n",
        "                                 help=\"if set multiplies predictions by this number\",\n",
        "                                 type=float,\n",
        "                                 default=1)\n",
        "        self.parser.add_argument(\"--ext_disp_to_eval\",\n",
        "                                 type=str,\n",
        "                                 help=\"optional path to a .npy disparities file to evaluate\")\n",
        "        self.parser.add_argument(\"--eval_split\",\n",
        "                                 type=str,\n",
        "                                 default=\"eigen\",\n",
        "                                 choices=[\n",
        "                                    \"eigen\", \"eigen_benchmark\", \"benchmark\", \"odom_9\", \"odom_10\"],\n",
        "                                 help=\"which split to run eval on\")\n",
        "        self.parser.add_argument(\"--save_pred_disps\",\n",
        "                                 help=\"if set saves predicted disparities\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--no_eval\",\n",
        "                                 help=\"if set disables evaluation\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--eval_eigen_to_benchmark\",\n",
        "                                 help=\"if set assume we are loading eigen results from npy but \"\n",
        "                                      \"we want to evaluate using the new benchmark.\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--eval_out_dir\",\n",
        "                                 help=\"if set will output the disparities to this folder\",\n",
        "                                 type=str)\n",
        "        self.parser.add_argument(\"--post_process\",\n",
        "                                 help=\"if set will perform the flipping post processing \"\n",
        "                                      \"from the original monodepth paper\",\n",
        "                                 action=\"store_true\")\n",
        "        self.parser.add_argument(\"--density\",\n",
        "                                 help=\"threshold create mask\",\n",
        "                                 type=float,\n",
        "                                 default=0.05)\n",
        "        self.parser.add_argument('--schedule', type=int, nargs='+', default=[8, 12],\n",
        "                        help='Decrease learning rate at these epochs.')\n",
        "        self.parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\n",
        "        self.parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
        "                        metavar='LR', help='initial learning rate')\n",
        "\n",
        "        self.parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                            help='momentum')\n",
        "\n",
        "        self.parser.add_argument('--weight-decay', '--wd', default=5.0e-4, type=float,\n",
        "                            metavar='W', help='weight decay (default: 1e-4)')\n",
        "        self.parser.add_argument('--image_path', type=str,\n",
        "                        help='path to a test image or folder of images')\n",
        "        self.parser.add_argument('--percent', default=0.5, type=float, help='percentage of weight to prune')\n",
        "\n",
        "    def parse(self):\n",
        "        self.options = self.parser.parse_args(args=[])\n",
        "        return self.options\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz2A59aGq_-n"
      },
      "source": [
        "\n",
        "options = MonodepthOptions()\n",
        "\n",
        "opts = options.parse()\n",
        "\n",
        "state = {k: v for k, v in opts._get_kwargs()}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq14OEUxn9Ze",
        "outputId": "001e495a-b93d-4fe6-a419-92e99b44c031"
      },
      "source": [
        "import datasets\n",
        "datasets_dict = {\"kitti\": datasets.KITTIRAWDataset,\n",
        "                         \"kitti_odom\": datasets.KITTIOdomDataset}\n",
        "dataset = datasets_dict['kitti']\n",
        "\n",
        "fpath = os.path.join(os.path.dirname(file_dir), \"splits\", opts.split, \"{}_files.txt\")\n",
        "\n",
        "train_filenames = readlines(fpath.format(\"train\"))\n",
        "val_filenames = readlines(fpath.format(\"val\"))\n",
        "img_ext = '.png'\n",
        "num_train_samples = len(train_filenames)\n",
        "num_total_steps = num_train_samples // opts.batch_size * opts.num_epochs\n",
        "\n",
        "train_dataset = dataset(\n",
        "    opts.data_path, train_filenames, opts.height, opts.width,\n",
        "    opts.frame_ids, 4, is_train=True, img_ext=img_ext)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, opts.batch_size, True,\n",
        "    num_workers=opts.num_workers, pin_memory=True, drop_last=True)\n",
        "val_dataset = dataset(\n",
        "    opts.data_path, val_filenames, opts.height, opts.width,\n",
        "    opts.frame_ids, 4, is_train=False, img_ext=img_ext)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, opts.batch_size, True,\n",
        "    num_workers=opts.num_workers, pin_memory=True, drop_last=True)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhbLzkXkwHIl"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYt8r55ioQQv"
      },
      "source": [
        "Class Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4noJD1cnkbi"
      },
      "source": [
        "import math\n",
        "class Trainer:\n",
        "    def __init__(self, options, train_loader=None, val_loader=None):\n",
        "        self.opt = options\n",
        "        self.log_path = os.path.join(self.opt.log_dir, self.opt.model_name)\n",
        "\n",
        "        # checking height and width are multiples of 32\n",
        "        assert self.opt.height % 32 == 0, \"'height' must be a multiple of 32\"\n",
        "        assert self.opt.width % 32 == 0, \"'width' must be a multiple of 32\"\n",
        "\n",
        "        self.models = {}\n",
        "        self.parameters_to_train = []\n",
        "        self.epoch = 0\n",
        "        self.step = 0\n",
        "        self.start_time = time.time()\n",
        "        self.device = torch.device(\"cpu\" if self.opt.no_cuda else \"cuda\")\n",
        "\n",
        "        self.num_scales = len(self.opt.scales)\n",
        "        self.num_input_frames = len(self.opt.frame_ids)\n",
        "        self.num_pose_frames = 2 if self.opt.pose_model_input == \"pairs\" else self.num_input_frames\n",
        "\n",
        "        assert self.opt.frame_ids[0] == 0, \"frame_ids must start with 0\"\n",
        "\n",
        "        self.use_pose_net = not (self.opt.use_stereo and self.opt.frame_ids == [0])\n",
        "\n",
        "        if self.opt.use_stereo:\n",
        "            self.opt.frame_ids.append(\"s\")\n",
        "\n",
        "        self.models[\"encoder\"] = networks.ResnetEncoder(\n",
        "            self.opt.num_layers, self.opt.weights_init == \"pretrained\")\n",
        "        self.models[\"encoder\"].to(self.device)\n",
        "        self.parameters_to_train += list(self.models[\"encoder\"].parameters())\n",
        "        self.models[\"depth\"] = networks.DepthDecoder(\n",
        "            self.models[\"encoder\"].num_ch_enc, self.opt.scales)\n",
        "        self.models[\"depth\"].to(self.device)\n",
        "        self.parameters_to_train += list(self.models[\"depth\"].parameters())\n",
        "\n",
        "        if self.use_pose_net:\n",
        "            if self.opt.pose_model_type == \"separate_resnet\":\n",
        "                self.models[\"pose_encoder\"] = networks.ResnetEncoder(\n",
        "                    self.opt.num_layers,\n",
        "                    self.opt.weights_init == \"pretrained\",\n",
        "                    num_input_images=self.num_pose_frames)\n",
        "\n",
        "                self.models[\"pose_encoder\"].to(self.device)\n",
        "                self.parameters_to_train += list(self.models[\"pose_encoder\"].parameters())\n",
        "\n",
        "                self.models[\"pose\"] = networks.PoseDecoder(\n",
        "                    self.models[\"pose_encoder\"].num_ch_enc,\n",
        "                    num_input_features=1,\n",
        "                    num_frames_to_predict_for=2)\n",
        "\n",
        "            elif self.opt.pose_model_type == \"shared\":\n",
        "                self.models[\"pose\"] = networks.PoseDecoder(\n",
        "                    self.models[\"encoder\"].num_ch_enc, self.num_pose_frames)\n",
        "\n",
        "            elif self.opt.pose_model_type == \"posecnn\":\n",
        "                self.models[\"pose\"] = networks.PoseCNN(\n",
        "                    self.num_input_frames if self.opt.pose_model_input == \"all\" else 2)\n",
        "\n",
        "            self.models[\"pose\"].to(self.device)\n",
        "            self.parameters_to_train += list(self.models[\"pose\"].parameters())\n",
        "\n",
        "        if self.opt.predictive_mask:\n",
        "            assert self.opt.disable_automasking, \\\n",
        "                \"When using predictive_mask, please disable automasking with --disable_automasking\"\n",
        "\n",
        "            # Our implementation of the predictive masking baseline has the the same architecture\n",
        "            # as our depth decoder. We predict a separate mask for each source frame.\n",
        "            self.models[\"predictive_mask\"] = networks.DepthDecoder(\n",
        "                self.models[\"encoder\"].num_ch_enc, self.opt.scales,\n",
        "                num_output_channels=(len(self.opt.frame_ids) - 1))\n",
        "            self.models[\"predictive_mask\"].to(self.device)\n",
        "            self.parameters_to_train += list(self.models[\"predictive_mask\"].parameters())\n",
        "\n",
        "        self.model_optimizer = optim.SGD(self.parameters_to_train, lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.weight_decay)\n",
        "        #self.model_optimizer = optim.Adam(self.parameters_to_train, self.opt.learning_rate)\n",
        "\n",
        "        self.model_lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "            self.model_optimizer, self.opt.scheduler_step_size, 0.1)\n",
        "\n",
        "        print(\"Training model named:\\n  \", self.opt.model_name)\n",
        "        print(\"Models and tensorboard events files are saved to:\\n  \", self.opt.log_dir)\n",
        "        print(\"Training is using:\\n  \", self.device)\n",
        "\n",
        "        # data\n",
        "        datasets_dict = {\"kitti\": datasets.KITTIRAWDataset,\n",
        "                         \"kitti_odom\": datasets.KITTIOdomDataset}\n",
        "        self.dataset = datasets_dict[self.opt.dataset]\n",
        "\n",
        "        fpath = os.path.join(os.path.dirname(file_dir), \"splits\", self.opt.split, \"{}_files.txt\")\n",
        "\n",
        "        train_filenames = readlines(fpath.format(\"train\"))\n",
        "        val_filenames = readlines(fpath.format(\"val\"))\n",
        "        img_ext = '.png' if self.opt.png else '.jpg'\n",
        "\n",
        "        num_train_samples = len(train_filenames)\n",
        "        self.num_total_steps = num_train_samples // self.opt.batch_size * self.opt.num_epochs\n",
        "\n",
        "        train_dataset = self.dataset(\n",
        "            self.opt.data_path, train_filenames, self.opt.height, self.opt.width,\n",
        "            self.opt.frame_ids, 4, is_train=True, img_ext=img_ext)\n",
        "        if train_loader is None:\n",
        "            self.train_loader = DataLoader(\n",
        "                train_dataset, self.opt.batch_size, True,\n",
        "                num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n",
        "        else:\n",
        "            self.train_loader = train_loader\n",
        "        val_dataset = self.dataset(\n",
        "            self.opt.data_path, val_filenames, self.opt.height, self.opt.width,\n",
        "            self.opt.frame_ids, 4, is_train=False, img_ext=img_ext)\n",
        "        if val_loader is None:\n",
        "            self.val_loader = DataLoader(\n",
        "                val_dataset, self.opt.batch_size, True,\n",
        "                num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n",
        "        else:\n",
        "            self.val_loader = val_loader\n",
        "        self.val_iter = iter(self.val_loader)\n",
        "        if self.opt.load_weights_folder is not None:\n",
        "            self.load_model()\n",
        "        self.writers = {}\n",
        "        for mode in [\"train\", \"val\"]:\n",
        "            self.writers[mode] = SummaryWriter(os.path.join(self.log_path, mode))\n",
        "\n",
        "        if not self.opt.no_ssim:\n",
        "            self.ssim = SSIM()\n",
        "            self.ssim.to(self.device)\n",
        "\n",
        "        self.backproject_depth = {}\n",
        "        self.project_3d = {}\n",
        "        for scale in self.opt.scales:\n",
        "            h = self.opt.height // (2 ** scale)\n",
        "            w = self.opt.width // (2 ** scale)\n",
        "\n",
        "            self.backproject_depth[scale] = BackprojectDepth(self.opt.batch_size, h, w)\n",
        "            self.backproject_depth[scale].to(self.device)\n",
        "\n",
        "            self.project_3d[scale] = Project3D(self.opt.batch_size, h, w)\n",
        "            self.project_3d[scale].to(self.device)\n",
        "\n",
        "        self.depth_metric_names = [\n",
        "            \"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n",
        "\n",
        "        print(\"Using split:\\n  \", self.opt.split)\n",
        "        print(\"There are {:d} training items and {:d} validation items\\n\".format(\n",
        "            len(train_dataset), len(val_dataset)))\n",
        "\n",
        "        self.save_opts()\n",
        "\n",
        "    def set_train(self):\n",
        "        \"\"\"Convert all models to training mode\n",
        "        \"\"\"\n",
        "        for m in self.models.values():\n",
        "            m.train()\n",
        "\n",
        "    def set_eval(self):\n",
        "        \"\"\"Convert all models to testing/evaluation mode\n",
        "        \"\"\"\n",
        "        for m in self.models.values():\n",
        "            m.eval()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Run the entire training pipeline\n",
        "        \"\"\"\n",
        "        self.epoch = 0\n",
        "        self.step = 0\n",
        "        self.start_time = time.time()\n",
        "        for self.epoch in range(self.opt.num_epochs):\n",
        "            self.run_epoch()\n",
        "            if (self.epoch + 1) % self.opt.save_frequency == 0:\n",
        "                self.save_model()\n",
        "\n",
        "    def run_epoch(self):\n",
        "        \"\"\"Run a single epoch of training and validation\n",
        "        \"\"\"\n",
        "        self.model_lr_scheduler.step()\n",
        "\n",
        "        print(\"Training\")\n",
        "        self.set_train()\n",
        "\n",
        "        for batch_idx, inputs in enumerate(self.train_loader):\n",
        "            print('index ', batch_idx)\n",
        "            before_op_time = time.time()\n",
        "\n",
        "            outputs, losses = self.process_batch(inputs)\n",
        "\n",
        "            self.model_optimizer.zero_grad()\n",
        "            losses[\"loss\"].backward()\n",
        "            self.model_optimizer.step()\n",
        "\n",
        "            duration = time.time() - before_op_time\n",
        "\n",
        "            # log less frequently after the first 2000 steps to save time & disk space\n",
        "            early_phase = batch_idx % self.opt.log_frequency == 0 and self.step < 2000\n",
        "            late_phase = self.step % 2000 == 0\n",
        "\n",
        "            if early_phase or late_phase:\n",
        "                self.log_time(batch_idx, duration, losses[\"loss\"].cpu().data)\n",
        "\n",
        "                if \"depth_gt\" in inputs:\n",
        "                    self.compute_depth_losses(inputs, outputs, losses)\n",
        "\n",
        "                self.log(\"train\", inputs, outputs, losses)\n",
        "                self.val()\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "    def process_batch(self, inputs):\n",
        "        \"\"\"Pass a minibatch through the network and generate images and losses\n",
        "        \"\"\"\n",
        "        for key, ipt in inputs.items():\n",
        "            inputs[key] = ipt.to(self.device)\n",
        "\n",
        "        if self.opt.pose_model_type == \"shared\":\n",
        "            # If we are using a shared encoder for both depth and pose (as advocated\n",
        "            # in monodepthv1), then all images are fed separately through the depth encoder.\n",
        "            all_color_aug = torch.cat([inputs[(\"color_aug\", i, 0)] for i in self.opt.frame_ids])\n",
        "            all_features = self.models[\"encoder\"](all_color_aug)\n",
        "            all_features = [torch.split(f, self.opt.batch_size) for f in all_features]\n",
        "\n",
        "            features = {}\n",
        "            for i, k in enumerate(self.opt.frame_ids):\n",
        "                features[k] = [f[i] for f in all_features]\n",
        "\n",
        "            outputs = self.models[\"depth\"](features[0])\n",
        "        else:\n",
        "            # Otherwise, we only feed the image with frame_id 0 through the depth encoder\n",
        "            features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
        "            outputs = self.models[\"depth\"](features)\n",
        "\n",
        "        if self.opt.predictive_mask:\n",
        "            outputs[\"predictive_mask\"] = self.models[\"predictive_mask\"](features)\n",
        "\n",
        "        if self.use_pose_net:\n",
        "            outputs.update(self.predict_poses(inputs, features))\n",
        "\n",
        "        self.generate_images_pred(inputs, outputs)\n",
        "        losses = self.compute_losses(inputs, outputs)\n",
        "\n",
        "        return outputs, losses\n",
        "\n",
        "    def predict_poses(self, inputs, features):\n",
        "        \"\"\"Predict poses between input frames for monocular sequences.\n",
        "        \"\"\"\n",
        "        outputs = {}\n",
        "        if self.num_pose_frames == 2:\n",
        "            # In this setting, we compute the pose to each source frame via a\n",
        "            # separate forward pass through the pose network.\n",
        "\n",
        "            # select what features the pose network takes as input\n",
        "            if self.opt.pose_model_type == \"shared\":\n",
        "                pose_feats = {f_i: features[f_i] for f_i in self.opt.frame_ids}\n",
        "            else:\n",
        "                pose_feats = {f_i: inputs[\"color_aug\", f_i, 0] for f_i in self.opt.frame_ids}\n",
        "\n",
        "            for f_i in self.opt.frame_ids[1:]:\n",
        "                if f_i != \"s\":\n",
        "                    # To maintain ordering we always pass frames in temporal order\n",
        "                    if f_i < 0:\n",
        "                        pose_inputs = [pose_feats[f_i], pose_feats[0]]\n",
        "                    else:\n",
        "                        pose_inputs = [pose_feats[0], pose_feats[f_i]]\n",
        "\n",
        "                    if self.opt.pose_model_type == \"separate_resnet\":\n",
        "                        pose_inputs = [self.models[\"pose_encoder\"](torch.cat(pose_inputs, 1))]\n",
        "                    elif self.opt.pose_model_type == \"posecnn\":\n",
        "                        pose_inputs = torch.cat(pose_inputs, 1)\n",
        "\n",
        "                    axisangle, translation = self.models[\"pose\"](pose_inputs)\n",
        "                    outputs[(\"axisangle\", 0, f_i)] = axisangle\n",
        "                    outputs[(\"translation\", 0, f_i)] = translation\n",
        "\n",
        "                    # Invert the matrix if the frame id is negative\n",
        "                    outputs[(\"cam_T_cam\", 0, f_i)] = transformation_from_parameters(\n",
        "                        axisangle[:, 0], translation[:, 0], invert=(f_i < 0))\n",
        "\n",
        "        else:\n",
        "            # Here we input all frames to the pose net (and predict all poses) together\n",
        "            if self.opt.pose_model_type in [\"separate_resnet\", \"posecnn\"]:\n",
        "                pose_inputs = torch.cat(\n",
        "                    [inputs[(\"color_aug\", i, 0)] for i in self.opt.frame_ids if i != \"s\"], 1)\n",
        "\n",
        "                if self.opt.pose_model_type == \"separate_resnet\":\n",
        "                    pose_inputs = [self.models[\"pose_encoder\"](pose_inputs)]\n",
        "\n",
        "            elif self.opt.pose_model_type == \"shared\":\n",
        "                pose_inputs = [features[i] for i in self.opt.frame_ids if i != \"s\"]\n",
        "\n",
        "            axisangle, translation = self.models[\"pose\"](pose_inputs)\n",
        "\n",
        "            for i, f_i in enumerate(self.opt.frame_ids[1:]):\n",
        "                if f_i != \"s\":\n",
        "                    outputs[(\"axisangle\", 0, f_i)] = axisangle\n",
        "                    outputs[(\"translation\", 0, f_i)] = translation\n",
        "                    outputs[(\"cam_T_cam\", 0, f_i)] = transformation_from_parameters(\n",
        "                        axisangle[:, i], translation[:, i])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate_images_pred(self, inputs, outputs):\n",
        "        \"\"\"Generate the warped (reprojected) color images for a minibatch.\n",
        "        Generated images are saved into the `outputs` dictionary.\n",
        "        \"\"\"\n",
        "        for scale in self.opt.scales:\n",
        "            disp = outputs[(\"disp\", scale)]\n",
        "            if self.opt.v1_multiscale:\n",
        "                source_scale = scale\n",
        "            else:\n",
        "                disp = F.interpolate(\n",
        "                    disp, [self.opt.height, self.opt.width], mode=\"bilinear\", align_corners=False)\n",
        "                source_scale = 0\n",
        "\n",
        "            _, depth = disp_to_depth(disp, self.opt.min_depth, self.opt.max_depth)\n",
        "\n",
        "            outputs[(\"depth\", 0, scale)] = depth\n",
        "\n",
        "            for i, frame_id in enumerate(self.opt.frame_ids[1:]):\n",
        "\n",
        "                if frame_id == \"s\":\n",
        "                    T = inputs[\"stereo_T\"]\n",
        "                else:\n",
        "                    T = outputs[(\"cam_T_cam\", 0, frame_id)]\n",
        "\n",
        "                # from the authors of https://arxiv.org/abs/1712.00175\n",
        "                if self.opt.pose_model_type == \"posecnn\":\n",
        "                    axisangle = outputs[(\"axisangle\", 0, frame_id)]\n",
        "                    translation = outputs[(\"translation\", 0, frame_id)]\n",
        "\n",
        "                    inv_depth = 1 / depth\n",
        "                    mean_inv_depth = inv_depth.mean(3, True).mean(2, True)\n",
        "\n",
        "                    T = transformation_from_parameters(\n",
        "                        axisangle[:, 0], translation[:, 0] * mean_inv_depth[:, 0], frame_id < 0)\n",
        "\n",
        "                cam_points = self.backproject_depth[source_scale](\n",
        "                    depth, inputs[(\"inv_K\", source_scale)])\n",
        "                pix_coords = self.project_3d[source_scale](\n",
        "                    cam_points, inputs[(\"K\", source_scale)], T)\n",
        "\n",
        "                outputs[(\"sample\", frame_id, scale)] = pix_coords\n",
        "\n",
        "                outputs[(\"color\", frame_id, scale)] = F.grid_sample(\n",
        "                    inputs[(\"color\", frame_id, source_scale)],\n",
        "                    outputs[(\"sample\", frame_id, scale)],\n",
        "                    padding_mode=\"border\")\n",
        "\n",
        "                if not self.opt.disable_automasking:\n",
        "                    outputs[(\"color_identity\", frame_id, scale)] = \\\n",
        "                        inputs[(\"color\", frame_id, source_scale)]\n",
        "\n",
        "    def compute_reprojection_loss(self, pred, target):\n",
        "        \"\"\"Computes reprojection loss between a batch of predicted and target images\n",
        "        \"\"\"\n",
        "        abs_diff = torch.abs(target - pred)\n",
        "        l1_loss = abs_diff.mean(1, True)\n",
        "\n",
        "        if self.opt.no_ssim:\n",
        "            reprojection_loss = l1_loss\n",
        "        else:\n",
        "            ssim_loss = self.ssim(pred, target).mean(1, True)\n",
        "            reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n",
        "\n",
        "        return reprojection_loss\n",
        "\n",
        "    def compute_losses(self, inputs, outputs):\n",
        "        \"\"\"Compute the reprojection and smoothness losses for a minibatch\n",
        "        \"\"\"\n",
        "        losses = {}\n",
        "        total_loss = 0\n",
        "\n",
        "        for scale in self.opt.scales:\n",
        "            loss = 0\n",
        "            reprojection_losses = []\n",
        "\n",
        "            if self.opt.v1_multiscale:\n",
        "                source_scale = scale\n",
        "            else:\n",
        "                source_scale = 0\n",
        "\n",
        "            disp = outputs[(\"disp\", scale)]\n",
        "            color = inputs[(\"color\", 0, scale)]\n",
        "            target = inputs[(\"color\", 0, source_scale)]\n",
        "\n",
        "            for frame_id in self.opt.frame_ids[1:]:\n",
        "                pred = outputs[(\"color\", frame_id, scale)]\n",
        "                reprojection_losses.append(self.compute_reprojection_loss(pred, target))\n",
        "\n",
        "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
        "\n",
        "            if not self.opt.disable_automasking:\n",
        "                identity_reprojection_losses = []\n",
        "                for frame_id in self.opt.frame_ids[1:]:\n",
        "                    pred = inputs[(\"color\", frame_id, source_scale)]\n",
        "                    identity_reprojection_losses.append(\n",
        "                        self.compute_reprojection_loss(pred, target))\n",
        "\n",
        "                identity_reprojection_losses = torch.cat(identity_reprojection_losses, 1)\n",
        "\n",
        "                if self.opt.avg_reprojection:\n",
        "                    identity_reprojection_loss = identity_reprojection_losses.mean(1, keepdim=True)\n",
        "                else:\n",
        "                    # save both images, and do min all at once below\n",
        "                    identity_reprojection_loss = identity_reprojection_losses\n",
        "\n",
        "            elif self.opt.predictive_mask:\n",
        "                # use the predicted mask\n",
        "                mask = outputs[\"predictive_mask\"][\"disp\", scale]\n",
        "                if not self.opt.v1_multiscale:\n",
        "                    mask = F.interpolate(\n",
        "                        mask, [self.opt.height, self.opt.width],\n",
        "                        mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "                reprojection_losses *= mask\n",
        "\n",
        "                # add a loss pushing mask to 1 (using nn.BCELoss for stability)\n",
        "                weighting_loss = 0.2 * nn.BCELoss()(mask, torch.ones(mask.shape).cuda())\n",
        "                loss += weighting_loss.mean()\n",
        "\n",
        "            if self.opt.avg_reprojection:\n",
        "                reprojection_loss = reprojection_losses.mean(1, keepdim=True)\n",
        "            else:\n",
        "                reprojection_loss = reprojection_losses\n",
        "\n",
        "            if not self.opt.disable_automasking:\n",
        "                # add random numbers to break ties\n",
        "                identity_reprojection_loss += torch.randn(\n",
        "                    identity_reprojection_loss.shape).cuda() * 0.00001\n",
        "\n",
        "                combined = torch.cat((identity_reprojection_loss, reprojection_loss), dim=1)\n",
        "            else:\n",
        "                combined = reprojection_loss\n",
        "\n",
        "            if combined.shape[1] == 1:\n",
        "                to_optimise = combined\n",
        "            else:\n",
        "                to_optimise, idxs = torch.min(combined, dim=1)\n",
        "\n",
        "            if not self.opt.disable_automasking:\n",
        "                outputs[\"identity_selection/{}\".format(scale)] = (\n",
        "                        idxs > identity_reprojection_loss.shape[1] - 1).float()\n",
        "\n",
        "            loss += to_optimise.mean()\n",
        "\n",
        "            mean_disp = disp.mean(2, True).mean(3, True)\n",
        "            norm_disp = disp / (mean_disp + 1e-7)\n",
        "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
        "\n",
        "            loss += self.opt.disparity_smoothness * smooth_loss / (2 ** scale)\n",
        "            total_loss += loss\n",
        "            losses[\"loss/{}\".format(scale)] = loss\n",
        "\n",
        "        total_loss /= self.num_scales\n",
        "        losses[\"loss\"] = total_loss\n",
        "        return losses\n",
        "\n",
        "    def compute_depth_losses(self, inputs, outputs, losses):\n",
        "        \"\"\"Compute depth metrics, to allow monitoring during training\n",
        "\n",
        "        This isn't particularly accurate as it averages over the entire batch,\n",
        "        so is only used to give an indication of validation performance\n",
        "        \"\"\"\n",
        "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
        "        depth_pred = torch.clamp(F.interpolate(\n",
        "            depth_pred, [375, 1242], mode=\"bilinear\", align_corners=False), 1e-3, 80)\n",
        "        depth_pred = depth_pred.detach()\n",
        "\n",
        "        depth_gt = inputs[\"depth_gt\"]\n",
        "        mask = depth_gt > 0\n",
        "\n",
        "        # garg/eigen crop\n",
        "        crop_mask = torch.zeros_like(mask)\n",
        "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
        "        mask = mask * crop_mask\n",
        "\n",
        "        depth_gt = depth_gt[mask]\n",
        "        depth_pred = depth_pred[mask]\n",
        "        depth_pred *= torch.median(depth_gt) / torch.median(depth_pred)\n",
        "\n",
        "        depth_pred = torch.clamp(depth_pred, min=1e-3, max=80)\n",
        "\n",
        "        depth_errors = compute_depth_errors(depth_gt, depth_pred)\n",
        "\n",
        "        for i, metric in enumerate(self.depth_metric_names):\n",
        "            losses[metric] = np.array(depth_errors[i].cpu())\n",
        "\n",
        "    def log_time(self, batch_idx, duration, loss):\n",
        "        \"\"\"Print a logging statement to the terminal\n",
        "        \"\"\"\n",
        "        samples_per_sec = self.opt.batch_size / duration\n",
        "        time_sofar = time.time() - self.start_time\n",
        "        training_time_left = (\n",
        "                                     self.num_total_steps / self.step - 1.0) * time_sofar if self.step > 0 else 0\n",
        "        print_string = \"epoch {:>3} | batch {:>6} | examples/s: {:5.1f}\" + \\\n",
        "                       \" | loss: {:.5f} | time elapsed: {} | time left: {}\"\n",
        "        print(print_string.format(self.epoch, batch_idx, samples_per_sec, loss,\n",
        "                                  sec_to_hm_str(time_sofar), sec_to_hm_str(training_time_left)))\n",
        "\n",
        "    def log(self, mode, inputs, outputs, losses):\n",
        "        \"\"\"Write an event to the tensorboard events file\n",
        "        \"\"\"\n",
        "        writer = self.writers[mode]\n",
        "        for l, v in losses.items():\n",
        "            writer.add_scalar(\"{}\".format(l), v, self.step)\n",
        "\n",
        "        for j in range(min(4, self.opt.batch_size)):  # write a maxmimum of four images\n",
        "            for s in self.opt.scales:\n",
        "                for frame_id in self.opt.frame_ids:\n",
        "                    writer.add_image(\n",
        "                        \"color_{}_{}/{}\".format(frame_id, s, j),\n",
        "                        inputs[(\"color\", frame_id, s)][j].data, self.step)\n",
        "                    if s == 0 and frame_id != 0:\n",
        "                        writer.add_image(\n",
        "                            \"color_pred_{}_{}/{}\".format(frame_id, s, j),\n",
        "                            outputs[(\"color\", frame_id, s)][j].data, self.step)\n",
        "\n",
        "                writer.add_image(\n",
        "                    \"disp_{}/{}\".format(s, j),\n",
        "                    normalize_image(outputs[(\"disp\", s)][j]), self.step)\n",
        "\n",
        "                if self.opt.predictive_mask:\n",
        "                    for f_idx, frame_id in enumerate(self.opt.frame_ids[1:]):\n",
        "                        writer.add_image(\n",
        "                            \"predictive_mask_{}_{}/{}\".format(frame_id, s, j),\n",
        "                            outputs[\"predictive_mask\"][(\"disp\", s)][j, f_idx][None, ...],\n",
        "                            self.step)\n",
        "\n",
        "                elif not self.opt.disable_automasking:\n",
        "                    writer.add_image(\n",
        "                        \"automask_{}/{}\".format(s, j),\n",
        "                        outputs[\"identity_selection/{}\".format(s)][j][None, ...], self.step)\n",
        "\n",
        "    def save_opts(self):\n",
        "        \"\"\"Save options to disk so we know what we ran this experiment with\n",
        "        \"\"\"\n",
        "        models_dir = os.path.join(self.log_path, \"models\")\n",
        "        if not os.path.exists(models_dir):\n",
        "            os.makedirs(models_dir)\n",
        "        to_save = self.opt.__dict__.copy()\n",
        "\n",
        "        with open(os.path.join(models_dir, 'opt.json'), 'w') as f:\n",
        "            json.dump(to_save, f, indent=2)\n",
        "\n",
        "    def save_model(self,typecur = \"models\"):\n",
        "        \"\"\"Save model weights to disk\n",
        "        \"\"\"\n",
        "        save_folder = os.path.join(self.log_path,typecur, \"weights_{}\".format(self.epoch))\n",
        "        print(save_folder)\n",
        "        if not os.path.exists(save_folder):\n",
        "            os.makedirs(save_folder)\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            save_path = os.path.join(save_folder, \"{}.pth\".format(model_name))\n",
        "            to_save = model.state_dict()\n",
        "            if model_name == 'encoder':\n",
        "                # save the sizes - these are needed at prediction time\n",
        "                to_save['height'] = self.opt.height\n",
        "                to_save['width'] = self.opt.width\n",
        "                to_save['use_stereo'] = self.opt.use_stereo\n",
        "            torch.save(to_save, save_path)\n",
        "\n",
        "        self.save_opt(typecur)\n",
        "\n",
        "    def save_opt(self,typecur = \"models\"):\n",
        "        save_folder = os.path.join(self.log_path, typecur, \"weights_{}\".format(self.epoch))\n",
        "        save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\n",
        "        torch.save(self.model_optimizer.state_dict(), save_path)\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model(s) from disk\n",
        "        \"\"\"\n",
        "        self.opt.load_weights_folder = os.path.expanduser(self.opt.load_weights_folder)\n",
        "\n",
        "        assert os.path.isdir(self.opt.load_weights_folder), \\\n",
        "            \"Cannot find folder {}\".format(self.opt.load_weights_folder)\n",
        "        print(\"loading model from folder {}\".format(self.opt.load_weights_folder))\n",
        "\n",
        "        for n in self.opt.models_to_load:\n",
        "            print(\"Loading {} weights...\".format(n))\n",
        "            path = os.path.join(self.opt.load_weights_folder, \"{}.pth\".format(n))\n",
        "            model_dict = self.models[n].state_dict()\n",
        "            pretrained_dict = torch.load(path)\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.models[n].load_state_dict(model_dict)\n",
        "\n",
        "        # loading adam state\n",
        "\n",
        "        optimizer_load_path = os.path.join(self.opt.load_weights_folder, \"adam.pth\")\n",
        "        print(optimizer_load_path)\n",
        "        if os.path.isfile(optimizer_load_path):\n",
        "            print(\"Loading Adam weights\")\n",
        "            optimizer_dict = torch.load(optimizer_load_path)\n",
        "            self.model_optimizer.load_state_dict(optimizer_dict)\n",
        "        else:\n",
        "            print(\"Cannot find Adam weights so Adam is randomly initialized\")\n",
        "            # self.init_optimizer()\n",
        "\n",
        "    def init_optimizer(self):\n",
        "        self.epoch = 0\n",
        "        self.step = 0\n",
        "        self.start_time = time.time()\n",
        "        for self.epoch in range(self.opt.num_epochs):\n",
        "            self.adjust_learning_rate()\n",
        "            self.model_lr_scheduler.step()\n",
        "            for batch_idx in range(len(self.train_loader)):\n",
        "                print('batch_idx:', batch_idx)\n",
        "                self.model_optimizer.zero_grad()\n",
        "\n",
        "                self.model_optimizer.step()\n",
        "\n",
        "    def val(self):\n",
        "        \"\"\"Validate the model on a single minibatch\n",
        "        \"\"\"\n",
        "        loss = AverageMeter('losses')\n",
        "\n",
        "        # top1 = AverageMeter('top1')\n",
        "        self.set_eval()\n",
        "        try:\n",
        "            inputs = self.val_iter.next()\n",
        "        except StopIteration:\n",
        "            self.val_iter = iter(self.val_loader)\n",
        "            inputs = self.val_iter.next()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, losses = self.process_batch(inputs)\n",
        "            # top1.update(outputs[('disp', 3)].data,inputs['depth_gt'].size(0))\n",
        "            print(losses)\n",
        "            if \"depth_gt\" in inputs:\n",
        "                self.compute_depth_losses(inputs, outputs, losses)\n",
        "            loss.update(losses['loss'].data, inputs['depth_gt'].size(0))\n",
        "            print(losses)\n",
        "\n",
        "            # self.log(\"val\", inputs, outputs, losses)\n",
        "            print('Result test: Test Loss:  %.8f ' % (loss.avg))\n",
        "            del inputs, outputs, losses\n",
        "\n",
        "        self.set_train()\n",
        "\n",
        "    def get_momentum_for_weight(self, weight):\n",
        "        # print('self.model_optimizer.state[weight]',self.model_optimizer.state.keys())\n",
        "        # grad = weight\n",
        "        if 'exp_avg' in self.model_optimizer.state[weight]:\n",
        "            adam_m1 = self.model_optimizer.state[weight]['exp_avg']\n",
        "            adam_m2 = self.model_optimizer.state[weight]['exp_avg_sq']\n",
        "            grad = adam_m1 / (torch.sqrt(adam_m2) + 1e-08)\n",
        "        elif 'momentum_buffer' in self.model_optimizer.state[weight]:\n",
        "            grad = self.model_optimizer.state[weight]['momentum_buffer']\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def momentum_redistribution(self, weight, mask):\n",
        "\n",
        "        grad = self.get_momentum_for_weight(weight)\n",
        "        mean_magnitude = torch.abs(grad[mask.bool()]).mean().item()\n",
        "\n",
        "        return mean_magnitude\n",
        "\n",
        "    def adjust_learning_rate(self):\n",
        "\n",
        "        if self.epoch in self.opt.schedule:\n",
        "            state['lr'] *= self.opt.gamma\n",
        "            for param_group in self.model_optimizer.param_groups:\n",
        "                param_group['lr'] = state['lr']\n",
        "\n",
        "    def pruning(self, mode='constant'):\n",
        "        if mode not in ['constant', 'optimize']:\n",
        "            return\n",
        "        # print(\"================Start evaluate origin=================\")\n",
        "        # self.val()\n",
        "\n",
        "        print(\"> start {} prune encoder\".format(mode))\n",
        "        if mode == 'constant':\n",
        "\n",
        "            self.constant_prunning(self.models['encoder'])\n",
        "        elif mode == 'optimize':\n",
        "            self.variance_prunning(self.models['encoder'])\n",
        "        print(\"================Start evaluate after {} pruning=================\".format(mode))\n",
        "        self.val()\n",
        "\n",
        "    def constant_prunning(self, model):\n",
        "\n",
        "        # pruning\n",
        "        total = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                total += m.weight.data.numel()\n",
        "        conv_weights = torch.zeros(total)\n",
        "        index = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                size = m.weight.data.numel()\n",
        "                conv_weights[index:(index + size)] = m.weight.data.view(-1).abs().clone()\n",
        "                index += size\n",
        "\n",
        "        y, i = torch.sort(conv_weights)\n",
        "        thre_index = int(total * self.opt.percent)\n",
        "        thre = y[thre_index]\n",
        "        pruned = 0\n",
        "        print('Pruning threshold: {}'.format(thre))\n",
        "        zero_flag = False\n",
        "        for k, m in enumerate(model.modules()):\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "\n",
        "                weight_copy = m.weight.data.abs().clone()\n",
        "                mask = weight_copy.gt(thre).float().cuda()\n",
        "                pruned = pruned + mask.numel() - torch.sum(mask)\n",
        "                m.weight.data.mul_(mask)\n",
        "                if int(torch.sum(mask)) == 0:\n",
        "                    zero_flag = True\n",
        "                # print('layer index: {:d} \\t total params: {:d} \\t remaining params: {:d}'.\n",
        "                #     format(k, mask.numel(), int(torch.sum(mask))))\n",
        "        print('Total conv params: {}, Pruned conv params: {}, Pruned ratio: {}'.format(total, pruned, pruned / total))\n",
        "\n",
        "        # self.val()\n",
        "\n",
        "    \n",
        "    def variance_prunning(self, model):\n",
        "        # pruning\n",
        "        total = 0\n",
        "        name2variance = {}\n",
        "        total_variance = 0.0\n",
        "        total_nonzero = 0\n",
        "        total_zero = 0\n",
        "        name2nonzeros = {}\n",
        "        name2zeros = {}\n",
        "        masks = {}\n",
        "        name = 0\n",
        "        name2prune_rate = {}\n",
        "\n",
        "        ### Lấy độ quan trọng của từng layer được thu qua quá trình training\n",
        "        for module in model.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "               \n",
        "\n",
        "                for curname, weight in module.named_parameters():\n",
        "                    \n",
        "                    masks[name] = torch.zeros_like(weight, dtype=torch.float32, requires_grad=False).cuda()\n",
        "                    masks[name][:] = (torch.rand(weight.shape) > self.opt.density).float().data.cuda()\n",
        "\n",
        "                    curmask = masks[name]\n",
        "                    #print(curmask)\n",
        "                    # redistribution\n",
        "                    name2variance[name] = self.momentum_redistribution(weight, curmask)\n",
        "\n",
        "                    if not np.isnan(name2variance[name]):\n",
        "                        total_variance += name2variance[name]\n",
        "                    name2nonzeros[name] = curmask.sum().item()\n",
        "                    name2zeros[name] = curmask.numel() - name2nonzeros[name]\n",
        "\n",
        "                    sparsity = name2zeros[name] / float(masks[name].numel())\n",
        "                    total_nonzero += name2nonzeros[name]\n",
        "                    total_zero += name2zeros[name]\n",
        "                    name += 1\n",
        "        ### Đưa độ quan trọng về thành %\n",
        "        for name in name2variance:\n",
        "            if total_variance != 0.0:\n",
        "                name2variance[name] /= total_variance\n",
        "        print('name2variance',name2variance.values())        \n",
        "        name = 0\n",
        "        ## Lấy giá trị lớn nhất và nhỏ nhất trong phần trăm quan trọng\n",
        "        max_ = max(list(name2variance.values()))\n",
        "        min_ = min(list(name2variance.values()))\n",
        "        thresholds = {}\n",
        "        \n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                total += m.weight.data.numel()\n",
        "        conv_weights = torch.zeros(total)\n",
        "        index = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                size = m.weight.data.numel()\n",
        "                conv_weights[index:(index + size)] = m.weight.data.view(-1).abs().clone()\n",
        "                index += size\n",
        "\n",
        "        y, i = torch.sort(conv_weights)\n",
        "        percent_pruned_actual = 0\n",
        "        thresh_index = 0\n",
        "\n",
        "        for name in name2variance:\n",
        "            ##Tính phần trăm thực sự sẽ prune dựa vào độ quan trọng\n",
        "            percent_pruned_actual = self.opt.percent * (max_ - name2variance[name]) / (max_ - min_)\n",
        "            ## tính index ngưỡng cắt đối với layer hiện tại\n",
        "            thresh_index = int(percent_pruned_actual * total) \n",
        "            \n",
        "            ## Lưu ngưỡng cắt đối với lớp hiện tại\n",
        "            thresholds[name] = y[thresh_index]\n",
        "            \n",
        "        pruned = 0\n",
        "        print('thresholds',thresholds)\n",
        "        name = 0\n",
        "        for k, m in enumerate(model.modules()):\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                for curname, weight in m.named_parameters():\n",
        "                    weight_copy = m.weight.data.abs().clone()\n",
        "                    ## cắt tỉa dựa vào ngưỡng cắt của từng lớp\n",
        "                    mask = weight_copy.gt(thresholds[name]).float().cuda()\n",
        "                    pruned = pruned + mask.numel() - torch.sum(mask)\n",
        "                    m.weight.data.mul_(mask)\n",
        "                    # print('layer index: {:d} \\t total params: {:d} \\t remaining params: {:d}'.\n",
        "                    #     format(k, curmask.numel(), int(torch.sum(curmask))))\n",
        "                    name += 1\n",
        "        print('Total conv params: {}, Pruned conv params: {}, Pruned ratio: {}'.format(total, pruned, pruned / total))\n",
        "        # self.val()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIcy9MwWYR5w"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C43zJgxrOEfy"
      },
      "source": [
        "##Evaluate origin model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYIXrQdbnzAl"
      },
      "source": [
        "!python evaluate_depth.py --load_weights_folder /content/drive/MyDrive/SSL/monodepth2/newmodel/mono_model/models/weights_3/ --eval_mono"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6s3yvoFOHyK"
      },
      "source": [
        "##Evaluate optimized model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPiqdRmJqAZI"
      },
      "source": [
        "trainer = Trainer(opts,train_loader,val_loader)\n",
        "\n",
        "trainer.pruning('optimize')\n",
        "trainer.save_model('pruned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5uoC2dXjyVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b64877c-78af-40e2-bc8b-fa8c0a35734d"
      },
      "source": [
        "\n",
        "!python evaluate_depth.py --load_weights_folder newmodel/newmodel/mono_model/models/weights_3/pruned/weights_0/ --eval_mono"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newmodel/newmodel/mono_model/models/weights_3/pruned/weights_0\n",
            "-> Loading weights from newmodel/newmodel/mono_model/models/weights_3/pruned/weights_0/\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "-> Computing predictions with size 640x192\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 32.546 | std: 0.127\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.196  &   1.454  &   6.556  &   0.273  &   0.690  &   0.892  &   0.962  \\\\\n",
            "\n",
            "-> Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF45tokz3gBw"
      },
      "source": [
        "##Evaluate constant pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Qmnfqo5cGq",
        "outputId": "c1022a4e-9a82-462b-c4fb-130c2455a401"
      },
      "source": [
        "trainer = Trainer(opts,train_loader,val_loader)\n",
        "trainer.pruning('constant')\n",
        "trainer.save_model('constant')\n",
        "!python evaluate_depth.py --load_weights_folder newmodel/newmodel/mono_model/models/weights_3/constant/weights_0/ --eval_mono"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model named:\n",
            "   newmodel/mono_model/models/weights_3\n",
            "Models and tensorboard events files are saved to:\n",
            "   newmodel\n",
            "Training is using:\n",
            "   cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model from folder newmodel/mono_model/models/weights_3\n",
            "Loading encoder weights...\n",
            "Loading depth weights...\n",
            "newmodel/mono_model/models/weights_3/adam.pth\n",
            "Loading Adam weights\n",
            "Using split:\n",
            "   eigen_zhou\n",
            "There are 39810 training items and 4424 validation items\n",
            "\n",
            "> start constant prune encoder\n",
            "Pruning threshold: 7.490741700166836e-05\n",
            "Total conv params: 11166912, Pruned conv params: 5583457.0, Pruned ratio: 0.5000000596046448\n",
            "================Start evaluate after constant pruning=================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss/0': tensor(0.1598, device='cuda:0'), 'loss/1': tensor(0.1597, device='cuda:0'), 'loss/2': tensor(0.1597, device='cuda:0'), 'loss/3': tensor(0.1596, device='cuda:0'), 'loss': tensor(0.1597, device='cuda:0')}\n",
            "{'loss/0': tensor(0.1598, device='cuda:0'), 'loss/1': tensor(0.1597, device='cuda:0'), 'loss/2': tensor(0.1597, device='cuda:0'), 'loss/3': tensor(0.1596, device='cuda:0'), 'loss': tensor(0.1597, device='cuda:0'), 'de/abs_rel': array(0.19452108, dtype=float32), 'de/sq_rel': array(1.5401381, dtype=float32), 'de/rms': array(7.1259727, dtype=float32), 'de/log_rms': array(0.29522824, dtype=float32), 'da/a1': array(0.68443865, dtype=float32), 'da/a2': array(0.878946, dtype=float32), 'da/a3': array(0.95371425, dtype=float32)}\n",
            "Result test: Test Loss:  0.15969440 \n",
            "newmodel/newmodel/mono_model/models/weights_3/constant/weights_0\n",
            "-> Loading weights from newmodel/newmodel/mono_model/models/weights_3/constant/weights_0/\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "-> Computing predictions with size 640x192\n",
            "-> Evaluating\n",
            "   Mono evaluation - using median scaling\n",
            " Scaling ratios | med: 32.547 | std: 0.127\n",
            "\n",
            "   abs_rel |   sq_rel |     rmse | rmse_log |       a1 |       a2 |       a3 | \n",
            "&   0.196  &   1.454  &   6.557  &   0.273  &   0.690  &   0.892  &   0.962  \\\\\n",
            "\n",
            "-> Done!\n"
          ]
        }
      ]
    }
  ]
}